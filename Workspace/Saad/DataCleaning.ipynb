{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6ac813",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b105d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:25:54.110072Z",
     "start_time": "2022-05-09T17:25:54.080061Z"
    }
   },
   "source": [
    "https://github.com/jasonwei20/eda_nlp/blob/04ab29c5b18d2d72f9fa5b304322aaf4793acea0/code/eda.py#L86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e7cdc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0fb176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.788073Z",
     "start_time": "2022-05-10T16:10:57.017672Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 50   # max displayed columns\n",
    "pd.options.display.max_colwidth = 280 # width of a column\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "import xgboost\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NB_n_jobs = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b205fce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.804078Z",
     "start_time": "2022-05-10T16:10:58.789073Z"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf1b269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.820080Z",
     "start_time": "2022-05-10T16:10:58.806077Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( '../../src' )\n",
    "from pandas_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73058b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.836084Z",
     "start_time": "2022-05-10T16:10:58.821081Z"
    }
   },
   "outputs": [],
   "source": [
    "dataFolder_path = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7876da2",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02fd7c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.852087Z",
     "start_time": "2022-05-10T16:10:58.837084Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     '''\n",
    "#     Translate nltk POS to wordnet tags\n",
    "#     '''\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN\n",
    "\n",
    "\n",
    "def doc_preparer(doc, stem = False, stop_words=sw, implement_transtranslate = False):\n",
    "    '''\n",
    "\n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    #Stemming seems to work better. Lemming can't identify plurals of products\n",
    "    \n",
    "    \n",
    "#     lemmed_keywords = ['apple',\n",
    "#                 'ipad', 'ipads',\n",
    "#                 'iphone', 'iphones',\n",
    "#                 'itunes',\n",
    "#                 'google', 'googled',\n",
    "#                 'android', 'droid', 'androids', 'droids',\n",
    "#                 'circle', 'circles'\n",
    "#                 'app', 'apps']\n",
    "\n",
    "#     stemmed_keywords = ['appl',\n",
    "#                         'ipad',\n",
    "#                         'iphon',\n",
    "#                         'itun',\n",
    "#                         'googl',\n",
    "#                         'android',\n",
    "#                         'droid',\n",
    "#                         'circl',\n",
    "#                         'app']\n",
    "\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "#     doc = pos_tag(doc)\n",
    "#     doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "#     doc = [word for word in doc if word in lemmed_keywords]\n",
    "\n",
    "#     if implement_transtranslate:\n",
    "#         doc = \n",
    "   \n",
    "    p_stemmer = nltk.stem.PorterStemmer()\n",
    "    if stem:\n",
    "        doc = [p_stemmer.stem(word) for word in doc]\n",
    "    return ' '.join(doc)\n",
    "\n",
    "\n",
    "def cv_printScores(cv_metric):\n",
    "    print('CV Results')\n",
    "    print('='*32)\n",
    "    print('Accuracy')\n",
    "    print('-'*32)\n",
    "    print(f\"Training accuracy: {cv_metric['train_accuracy'].mean():.3f}\")\n",
    "    print(f\"Test accuracy:     {cv_metric['test_accuracy'].mean():.3f}\")\n",
    "    print('F-1 Score')\n",
    "    print('-'*32)\n",
    "    print(f\"Training F1 score: {cv_metric['train_f1_macro'].mean():.3f}\")\n",
    "    print(f\"Test F1 score:     {cv_metric['test_f1_macro'].mean():.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6c08fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.868090Z",
     "start_time": "2022-05-10T16:10:58.853088Z"
    }
   },
   "outputs": [],
   "source": [
    "# from googletrans import Translator\n",
    "# translator = Translator()\n",
    "\n",
    "# def German_translation(x):\n",
    "#     print(x)    \n",
    "#     german_translation = translator.translate(x, dest='de')    \n",
    "#     return german_translation.text\n",
    "\n",
    "# def English_translation(x):\n",
    "#     print(x)    \n",
    "#     english_translation = translator.translate(x, dest='en')    \n",
    "#     return english_translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea6ad7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:58.884094Z",
     "start_time": "2022-05-10T16:10:58.869091Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Original Author: Jason Wei\n",
    "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks:\n",
    "https://github.com/jasonwei20/eda_nlp/blob/04ab29c5b18d2d72f9fa5b304322aaf4793acea0/code/eda.py#L86\n",
    "\n",
    "'''\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "#             print(l.name())\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\n",
    "            if (synonym in sw):\n",
    "                pass\n",
    "            synonyms.add(synonym) \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "def synonym_augmentation(sentence, numOfWordsToSyn=1, numOfExtraSentences=2):\n",
    "    '''\n",
    "    sentence: string to augment using synomization\n",
    "    numOfWordsToSyn: number of words in the sentence to synonymize\n",
    "    numOfExtraSentences: number of augmented sentences to return. \n",
    "    '''\n",
    "    new_sentences = []\n",
    "    sentence = sentence.lower()\n",
    "    original_words = sentence.split()\n",
    "    \n",
    "    if len(original_words)<numOfWordsToSyn:\n",
    "        numOfWordsToSyn = len(original_words)\n",
    "        \n",
    "    random_word_list = list(set(original_words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    # Randomly pick a word in list \n",
    "    # Pick a random synonym that is not in sw list\n",
    "    # Replace that word in our list of words    \n",
    "    for i in range(0,numOfExtraSentences):\n",
    "        new_words = original_words\n",
    "        for random_word in random_word_list:\n",
    "            synonyms = get_synonyms(random_word)\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(list(synonyms))\n",
    "\n",
    "                #new_words is rebuilt with synonym replacement, while maintaining order of words\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                random_word_list.remove(random_word)\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= numOfWordsToSyn: #only replace up to n words\n",
    "                break\n",
    "                \n",
    "        #Form new sentence by joining new words\n",
    "        s = ' '.join(new_words).lower().strip().replace('  ', ' ')\n",
    "        \n",
    "        # add sentences to our list of synonymized sentences\n",
    "        # if not already there and not same as the original sentence\n",
    "        if (s not in new_sentences) & (s!=sentence):\n",
    "            new_sentences.append(s)\n",
    "\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b1b552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:59.944336Z",
     "start_time": "2022-05-10T16:10:58.885095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"holy place shit, i can't trust this whole shebang\",\n",
       " \"holy shit, ace can't believe this works\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonym_augmentation(\"holy shit, I can't believe this works\",numOfWordsToSyn=5,numOfExtraSentences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601621e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:10:59.976343Z",
     "start_time": "2022-05-10T16:10:59.946337Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dataFolder_path+'judge_1377884607_tweet_product_company.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29a914f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:00.054360Z",
     "start_time": "2022-05-10T16:10:59.977343Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datframe has 8721 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Info Table:</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Zeroes</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Nulls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Uniques</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Missing/Unknown</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Details:</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Columns:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweet_text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01 %</td>\n",
       "      <td>8694</td>\n",
       "      <td>99.69 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>5552</td>\n",
       "      <td>63.66 %</td>\n",
       "      <td>10</td>\n",
       "      <td>0.11 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Info Table:                                        Zeroes          Nulls  \\\n",
       "Details:                                            Count Fraction Count   \n",
       "Columns:                                                                   \n",
       "tweet_text                                              0   0.00 %     1   \n",
       "emotion_in_tweet_is_directed_at                         0   0.00 %  5552   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product      0   0.00 %     0   \n",
       "\n",
       "Info Table:                                                 Uniques           \\\n",
       "Details:                                           Fraction   Count Fraction   \n",
       "Columns:                                                                       \n",
       "tweet_text                                           0.01 %    8694  99.69 %   \n",
       "emotion_in_tweet_is_directed_at                     63.66 %      10   0.11 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product   0.00 %       4   0.05 %   \n",
       "\n",
       "Info Table:                                        Missing/Unknown           \\\n",
       "Details:                                                     Count Fraction   \n",
       "Columns:                                                                      \n",
       "tweet_text                                                       0   0.00 %   \n",
       "emotion_in_tweet_is_directed_at                                  0   0.00 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product               0   0.00 %   \n",
       "\n",
       "Info Table:                                        Mean Median  \n",
       "Details:                                                        \n",
       "Columns:                                                        \n",
       "tweet_text                                          0.0    0.0  \n",
       "emotion_in_tweet_is_directed_at                     0.0    0.0  \n",
       "is_there_an_emotion_directed_at_a_brand_or_product  0.0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataFrame_info(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b8eb3",
   "metadata": {},
   "source": [
    "Looking at the 1 null in tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f05431b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:00.069363Z",
     "start_time": "2022-05-10T16:11:00.056361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_text emotion_in_tweet_is_directed_at  \\\n",
       "6        NaN                             NaN   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "6                 No emotion toward brand or product  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df.tweet_text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ca7a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:00.084368Z",
     "start_time": "2022-05-10T16:11:00.070364Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df.dropna(subset=['tweet_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "675d5b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:00.099370Z",
     "start_time": "2022-05-10T16:11:00.086368Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8720, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab20409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:01.678729Z",
     "start_time": "2022-05-10T16:11:00.100370Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datframe has 8720 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Info Table:</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Zeroes</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Nulls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Uniques</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Missing/Unknown</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Details:</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Columns:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweet_text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>8693</td>\n",
       "      <td>99.69 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>5551</td>\n",
       "      <td>63.66 %</td>\n",
       "      <td>10</td>\n",
       "      <td>0.11 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Info Table:                                        Zeroes          Nulls  \\\n",
       "Details:                                            Count Fraction Count   \n",
       "Columns:                                                                   \n",
       "tweet_text                                              0   0.00 %     0   \n",
       "emotion_in_tweet_is_directed_at                         0   0.00 %  5551   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product      0   0.00 %     0   \n",
       "\n",
       "Info Table:                                                 Uniques           \\\n",
       "Details:                                           Fraction   Count Fraction   \n",
       "Columns:                                                                       \n",
       "tweet_text                                           0.00 %    8693  99.69 %   \n",
       "emotion_in_tweet_is_directed_at                     63.66 %      10   0.11 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product   0.00 %       4   0.05 %   \n",
       "\n",
       "Info Table:                                        Missing/Unknown           \\\n",
       "Details:                                                     Count Fraction   \n",
       "Columns:                                                                      \n",
       "tweet_text                                                       0   0.00 %   \n",
       "emotion_in_tweet_is_directed_at                                  0   0.00 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product               0   0.00 %   \n",
       "\n",
       "Info Table:                                        Mean Median  \n",
       "Details:                                                        \n",
       "Columns:                                                        \n",
       "tweet_text                                          0.0    0.0  \n",
       "emotion_in_tweet_is_directed_at                     0.0    0.0  \n",
       "is_there_an_emotion_directed_at_a_brand_or_product  0.0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataFrame_info(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d67f45",
   "metadata": {},
   "source": [
    "Lets look at the emotion quotient column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591fca67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:01.694733Z",
     "start_time": "2022-05-10T16:11:01.679729Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5155\n",
       "Positive emotion                      2869\n",
       "Negative emotion                       545\n",
       "I can't tell                           151\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c4048",
   "metadata": {},
   "source": [
    "Major class imbalance. Should consider dropping \"I can't tell\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efa191cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:01.710736Z",
     "start_time": "2022-05-10T16:11:01.695734Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_df[data_df.is_there_an_emotion_directed_at_a_brand_or_product == \"I can't tell\" ]\n",
    "\n",
    "data_df = data_df[data_df.is_there_an_emotion_directed_at_a_brand_or_product != \"I can't tell\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdbe87a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:01.725740Z",
     "start_time": "2022-05-10T16:11:01.711735Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8569, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3da51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T02:36:50.450837Z",
     "start_time": "2022-05-09T02:36:50.432834Z"
    }
   },
   "source": [
    "Lets look at some of the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc43408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:02.058814Z",
     "start_time": "2022-05-10T16:11:01.726740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 9116),\n",
       " ('mention', 6851),\n",
       " ('link', 4077),\n",
       " ('rt', 2925),\n",
       " ('ipad', 2848),\n",
       " ('google', 2504),\n",
       " ('apple', 2184),\n",
       " ('quot', 1582),\n",
       " ('iphone', 1497),\n",
       " ('store', 1399),\n",
       " ('new', 1057),\n",
       " ('austin', 921),\n",
       " ('amp', 803),\n",
       " ('app', 792),\n",
       " ('circles', 639),\n",
       " ('social', 633),\n",
       " ('launch', 628),\n",
       " ('today', 566),\n",
       " ('android', 565),\n",
       " ('pop', 543),\n",
       " ('network', 447),\n",
       " ('via', 400),\n",
       " ('line', 391),\n",
       " ('get', 383),\n",
       " ('free', 378),\n",
       " ('called', 353),\n",
       " ('mobile', 342),\n",
       " ('party', 335),\n",
       " ('sxswi', 333),\n",
       " ('major', 301),\n",
       " ('one', 297),\n",
       " ('like', 275),\n",
       " ('time', 262),\n",
       " ('w', 261),\n",
       " ('check', 257),\n",
       " ('temporary', 254),\n",
       " ('opening', 242),\n",
       " ('possibly', 240),\n",
       " ('day', 231),\n",
       " ('people', 223),\n",
       " ('see', 217),\n",
       " ('downtown', 216),\n",
       " ('mayer', 212),\n",
       " ('great', 211),\n",
       " ('going', 211),\n",
       " ('maps', 211),\n",
       " ('apps', 210),\n",
       " ('go', 203),\n",
       " ('popup', 198),\n",
       " ('need', 196)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = FreqDist()\n",
    "for tweet in data_df['tweet_text'].map(lambda x:doc_preparer(x,stem=False)):\n",
    "    for word in tweet.split():\n",
    "        word_freq[word] +=1\n",
    "word_freq.most_common(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccffee",
   "metadata": {},
   "source": [
    "Adding venue specific words and twitter specific words to stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc3a00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:02.073818Z",
     "start_time": "2022-05-10T16:11:02.059814Z"
    }
   },
   "outputs": [],
   "source": [
    "#Maybe don't add mention? and link?\n",
    "sw.extend(['sxsw','rt','quot','austin','sxswi',\n",
    "           'mention','link',\n",
    "           'today','w'\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bed540e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:03.307098Z",
     "start_time": "2022-05-10T16:11:02.074818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipad', 2935),\n",
       " ('googl', 2508),\n",
       " ('appl', 2187),\n",
       " ('iphon', 1505),\n",
       " ('store', 1437),\n",
       " ('new', 1057),\n",
       " ('app', 1002),\n",
       " ('amp', 803),\n",
       " ('launch', 802),\n",
       " ('circl', 654),\n",
       " ('social', 637),\n",
       " ('android', 565),\n",
       " ('pop', 558),\n",
       " ('get', 514),\n",
       " ('open', 498),\n",
       " ('network', 468),\n",
       " ('line', 440),\n",
       " ('go', 416),\n",
       " ('via', 400),\n",
       " ('call', 389),\n",
       " ('parti', 387),\n",
       " ('free', 378),\n",
       " ('mobil', 345),\n",
       " ('come', 326),\n",
       " ('like', 309),\n",
       " ('use', 309),\n",
       " ('major', 306),\n",
       " ('win', 305),\n",
       " ('time', 304),\n",
       " ('one', 301),\n",
       " ('check', 300),\n",
       " ('day', 280),\n",
       " ('map', 264),\n",
       " ('possibl', 254),\n",
       " ('temporari', 254),\n",
       " ('see', 250),\n",
       " ('need', 238),\n",
       " ('look', 228),\n",
       " ('design', 225),\n",
       " ('peopl', 223),\n",
       " ('make', 219),\n",
       " ('downtown', 216),\n",
       " ('mayer', 213),\n",
       " ('great', 211),\n",
       " ('popup', 199),\n",
       " ('know', 196),\n",
       " ('marissa', 186),\n",
       " ('talk', 184),\n",
       " ('think', 182),\n",
       " ('set', 181)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = FreqDist()\n",
    "for tweet in data_df['tweet_text'].map(lambda x:doc_preparer(x,stem=True)):\n",
    "    for word in tweet.split():\n",
    "        word_freq[word] +=1\n",
    "word_freq.most_common(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e4fd93a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.589389Z",
     "start_time": "2022-05-10T16:11:03.308098Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df['stemmed_tokens'] = data_df['tweet_text'].map(lambda x:doc_preparer(x,stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfb76f15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.605392Z",
     "start_time": "2022-05-10T16:11:04.590389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley g iphon hr tweet rise dead need upgrad plugin station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessede know fludapp awesom ipad iphon app like appreci design also give free ts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festiv crashi year iphon app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer googl tim reilli tech book confer amp matt mullenweg wordpress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>ipad everywher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>wave buzz interrupt regularli schedul geek program big news googl circl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &amp;quot;We're operating w/out data.&amp;quot; #sxsw #health2dev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>googl zeiger physician never report potenti ae yet fda reli physician oper data health dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>verizon iphon custom complain time fell back hour weekend cours new yorker attend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>�ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>googl test check offer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet_text  \\\n",
       "0                       .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1           @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                                       @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                                    @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4                   @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "...                                                                                                                                                 ...   \n",
       "8716                                                                                                                      Ipad everywhere. #SXSW {link}   \n",
       "8717                      Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles   \n",
       "8718  Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev   \n",
       "8719       Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.   \n",
       "8720                                                  �ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "8716                            iPad   \n",
       "8717                             NaN   \n",
       "8718                             NaN   \n",
       "8719                             NaN   \n",
       "8720                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                      Negative emotion   \n",
       "1                                      Positive emotion   \n",
       "2                                      Positive emotion   \n",
       "3                                      Negative emotion   \n",
       "4                                      Positive emotion   \n",
       "...                                                 ...   \n",
       "8716                                   Positive emotion   \n",
       "8717                 No emotion toward brand or product   \n",
       "8718                 No emotion toward brand or product   \n",
       "8719                 No emotion toward brand or product   \n",
       "8720                 No emotion toward brand or product   \n",
       "\n",
       "                                                                                              stemmed_tokens  \n",
       "0                                               wesley g iphon hr tweet rise dead need upgrad plugin station  \n",
       "1                           jessede know fludapp awesom ipad iphon app like appreci design also give free ts  \n",
       "2                                                                             swonderlin wait ipad also sale  \n",
       "3                                                                     hope year festiv crashi year iphon app  \n",
       "4     sxtxstate great stuff fri marissa mayer googl tim reilli tech book confer amp matt mullenweg wordpress  \n",
       "...                                                                                                      ...  \n",
       "8716                                                                                          ipad everywher  \n",
       "8717                                 wave buzz interrupt regularli schedul geek program big news googl circl  \n",
       "8718              googl zeiger physician never report potenti ae yet fda reli physician oper data health dev  \n",
       "8719                       verizon iphon custom complain time fell back hour weekend cours new yorker attend  \n",
       "8720                                                                                  googl test check offer  \n",
       "\n",
       "[8569 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3baaef",
   "metadata": {},
   "source": [
    "# Label Encoding the sentinment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1c279ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.621397Z",
     "start_time": "2022-05-10T16:11:04.606393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'No emotion toward brand or product',\n",
       "       'Positive emotion'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "data_df['sentiment_target'] = le.fit_transform(data_df.is_there_an_emotion_directed_at_a_brand_or_product)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6903ec0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.637399Z",
     "start_time": "2022-05-10T16:11:04.622396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>sentiment_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley g iphon hr tweet rise dead need upgrad plugin station</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessede know fludapp awesom ipad iphon app like appreci design also give free ts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait ipad also sale</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festiv crashi year iphon app</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer googl tim reilli tech book confer amp matt mullenweg wordpress</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>ipad everywher</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>wave buzz interrupt regularli schedul geek program big news googl circl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &amp;quot;We're operating w/out data.&amp;quot; #sxsw #health2dev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>googl zeiger physician never report potenti ae yet fda reli physician oper data health dev</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>verizon iphon custom complain time fell back hour weekend cours new yorker attend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>�ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>googl test check offer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet_text  \\\n",
       "0                       .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1           @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                                       @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                                    @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4                   @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "...                                                                                                                                                 ...   \n",
       "8716                                                                                                                      Ipad everywhere. #SXSW {link}   \n",
       "8717                      Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles   \n",
       "8718  Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev   \n",
       "8719       Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.   \n",
       "8720                                                  �ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "8716                            iPad   \n",
       "8717                             NaN   \n",
       "8718                             NaN   \n",
       "8719                             NaN   \n",
       "8720                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                      Negative emotion   \n",
       "1                                      Positive emotion   \n",
       "2                                      Positive emotion   \n",
       "3                                      Negative emotion   \n",
       "4                                      Positive emotion   \n",
       "...                                                 ...   \n",
       "8716                                   Positive emotion   \n",
       "8717                 No emotion toward brand or product   \n",
       "8718                 No emotion toward brand or product   \n",
       "8719                 No emotion toward brand or product   \n",
       "8720                 No emotion toward brand or product   \n",
       "\n",
       "                                                                                              stemmed_tokens  \\\n",
       "0                                               wesley g iphon hr tweet rise dead need upgrad plugin station   \n",
       "1                           jessede know fludapp awesom ipad iphon app like appreci design also give free ts   \n",
       "2                                                                             swonderlin wait ipad also sale   \n",
       "3                                                                     hope year festiv crashi year iphon app   \n",
       "4     sxtxstate great stuff fri marissa mayer googl tim reilli tech book confer amp matt mullenweg wordpress   \n",
       "...                                                                                                      ...   \n",
       "8716                                                                                          ipad everywher   \n",
       "8717                                 wave buzz interrupt regularli schedul geek program big news googl circl   \n",
       "8718              googl zeiger physician never report potenti ae yet fda reli physician oper data health dev   \n",
       "8719                       verizon iphon custom complain time fell back hour weekend cours new yorker attend   \n",
       "8720                                                                                  googl test check offer   \n",
       "\n",
       "      sentiment_target  \n",
       "0                    0  \n",
       "1                    2  \n",
       "2                    2  \n",
       "3                    0  \n",
       "4                    2  \n",
       "...                ...  \n",
       "8716                 2  \n",
       "8717                 1  \n",
       "8718                 1  \n",
       "8719                 1  \n",
       "8720                 1  \n",
       "\n",
       "[8569 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a57a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76321443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6adb46d4",
   "metadata": {},
   "source": [
    "# Define X,y, train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe31240d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.653403Z",
     "start_time": "2022-05-10T16:11:04.638400Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data_df['stemmed_tokens']\n",
    "y = data_df['sentiment_target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ca071c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.669408Z",
     "start_time": "2022-05-10T16:11:04.654403Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6864"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_freq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "450a51a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.685411Z",
     "start_time": "2022-05-10T16:11:04.672407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess android app time\n",
      "['gauge android app clock time', 'guess humanoid app time']\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(X_train, y_train, right_index = True,\n",
    "               left_index = True)\n",
    "df[df['sentiment_target'] == 0]\n",
    "for each in (df[df['sentiment_target'] == 0]).stemmed_tokens:\n",
    "    print(each)\n",
    "    print(synonym_augmentation(each,numOfWordsToSyn=5))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42635827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.700414Z",
     "start_time": "2022-05-10T16:11:04.686411Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3642\n",
       "2    1963\n",
       "0     393\n",
       "Name: sentiment_target, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7b78ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.716417Z",
     "start_time": "2022-05-10T16:11:04.701414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7017                                     anyon know appl store still ipad like run\n",
       "2470                                      realli googl go call circl hmm see later\n",
       "8131               thing got smarmcak writeup googl ontologist ontologyshoutoutwut\n",
       "2655       get readi session design ipad interfac lynn teo ballroom convent center\n",
       "3664                                  line alreadi form temp appl store open insan\n",
       "                                           ...                                    \n",
       "1455        ok come build custom map googl map night organ parti itinerari product\n",
       "6078                                 media circu guy walk appl popup store st ipad\n",
       "752                       googl launch major new social network call circl possibl\n",
       "2720    save tree go cardless sign cardless contact tool environment android iphon\n",
       "928                               offici app go android iphon ipad lt thank instal\n",
       "Name: stemmed_tokens, Length: 5998, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025cae",
   "metadata": {},
   "source": [
    "# CountVec with MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e36304c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:04.908461Z",
     "start_time": "2022-05-10T16:11:04.717418Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb885734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:09.489500Z",
     "start_time": "2022-05-10T16:11:04.909463Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.826\n",
      "Test accuracy:     0.654\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.740\n",
      "Test F1 score:     0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    4.3s finished\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec,y_train)\n",
    "cvec_mnb_cvResults = cross_validate(mnb,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      cv=4,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(cvec_mnb_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93010be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:09.743557Z",
     "start_time": "2022-05-10T16:11:09.490500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6549980552314275"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec = cvec.transform(X_test)\n",
    "X_test_vec = pd.DataFrame.sparse.from_spmatrix(X_test_vec)\n",
    "X_test_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_test_vec.set_index(y_test.index, inplace=True)\n",
    "\n",
    "mnb.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9053e23",
   "metadata": {},
   "source": [
    "# TiffyDiffy with MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2356535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:10.015621Z",
     "start_time": "2022-05-10T16:11:09.744558Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(tvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1641a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:13.927507Z",
     "start_time": "2022-05-10T16:11:10.016621Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.742\n",
      "Test accuracy:     0.654\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.486\n",
      "Test F1 score:     0.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    3.6s finished\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec,y_train)\n",
    "tvec_mnb_cvResults = cross_validate(mnb,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      cv=4,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(tvec_mnb_cvResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18944db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T15:48:34.339194Z",
     "start_time": "2022-05-10T15:48:34.330192Z"
    }
   },
   "source": [
    "# CountVec with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bded004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:14.230575Z",
     "start_time": "2022-05-10T16:11:13.928507Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64d3c4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:29.175966Z",
     "start_time": "2022-05-10T16:11:14.231575Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.826\n",
      "Test accuracy:     0.654\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.740\n",
      "Test F1 score:     0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:   10.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vec,y_train)\n",
    "cvec_rfc_cvResults = cross_validate(rfc,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      cv=4,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(cvec_mnb_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b404205f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:29.734091Z",
     "start_time": "2022-05-10T16:11:29.177966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6818358615324777"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec = cvec.transform(X_test)\n",
    "X_test_vec = pd.DataFrame.sparse.from_spmatrix(X_test_vec)\n",
    "X_test_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_test_vec.set_index(y_test.index, inplace=True)\n",
    "\n",
    "rfc.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c219744",
   "metadata": {},
   "source": [
    "# TiffyDiffy with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0a643d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:29.814110Z",
     "start_time": "2022-05-10T16:11:29.735092Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(tvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "188c910a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:44.317401Z",
     "start_time": "2022-05-10T16:11:29.815110Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.968\n",
      "Test accuracy:     0.679\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.964\n",
      "Test F1 score:     0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    9.7s finished\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vec,y_train)\n",
    "tvec_rfc_cvResults = cross_validate(rfc,\n",
    "                                    X_train_vec,\n",
    "                                    y_train,\n",
    "                                    scoring=('accuracy', 'f1_macro'),\n",
    "                                    cv=4,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=NB_n_jobs,\n",
    "                                    return_train_score=True)\n",
    "\n",
    "cv_printScores(tvec_rfc_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1dca96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:44.332404Z",
     "start_time": "2022-05-10T16:11:44.318401Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "# params['rfc__criterion'] = ['gini','entropy']\n",
    "# params['rfc__n_estimators'] = np.arange(50,250,50)\n",
    "# params['rfc__max_depth'] = np.arange(150,200,10)\n",
    "# # params['rfc__max_leaf_nodes']=[4000,4500,5000]\n",
    "\n",
    "# rfc_model_pipe = Pipeline([\n",
    "#     ('vec',TfidfVectorizer()),\n",
    "#    ('rfc',RandomForestClassifier(random_state=42,n_jobs=NB_n_jobs))\n",
    "# ])\n",
    "\n",
    "# rfc_gs1 = GridSearchCV(estimator=rfc_model_pipe,\n",
    "#                            param_grid=params,\n",
    "#                            n_jobs=NB_n_jobs,\n",
    "#                            scoring=[ 'accuracy','precision_macro','recall_macro','f1_macro'],\n",
    "#                            refit='accuracy',\n",
    "#                            return_train_score=True)\n",
    "\n",
    "# rfc_gs1.fit(X_train,y_train)\n",
    "# prettyPrintGridCVResults(rfc_gs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13814e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:44.348407Z",
     "start_time": "2022-05-10T16:11:44.334404Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_gs1.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa965e30",
   "metadata": {},
   "source": [
    "# TiffyDiffy with GradientBoosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1c19415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:44.428426Z",
     "start_time": "2022-05-10T16:11:44.349408Z"
    }
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(tvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ba3b1f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T16:11:59.630088Z",
     "start_time": "2022-05-10T16:11:44.429425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.737\n",
      "Test accuracy:     0.670\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.571\n",
      "Test F1 score:     0.446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:   11.3s finished\n"
     ]
    }
   ],
   "source": [
    "grad = GradientBoostingClassifier()\n",
    "grad.fit(X_train_vec,y_train)\n",
    "tvec_grad_cvResults = cross_validate(grad,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(tvec_grad_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc0c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ceae97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
