{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6ac813",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b105d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T17:25:54.110072Z",
     "start_time": "2022-05-09T17:25:54.080061Z"
    }
   },
   "source": [
    "https://github.com/jasonwei20/eda_nlp/blob/04ab29c5b18d2d72f9fa5b304322aaf4793acea0/code/eda.py#L86"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e7cdc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "de0fb176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:54:33.731562Z",
     "start_time": "2022-05-10T17:54:33.726570Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 50   # max displayed columns\n",
    "pd.options.display.max_colwidth = 280 # width of a column\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "from googletrans import Translator\n",
    "\n",
    "import xgboost\n",
    "# from imblearn.over_sampling import SMOTE \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NB_n_jobs = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b205fce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:54:33.953655Z",
     "start_time": "2022-05-10T17:54:33.938617Z"
    }
   },
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dbf1b269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:54:34.157914Z",
     "start_time": "2022-05-10T17:54:34.143701Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( '../../src' )\n",
    "from pandas_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "73058b5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:54:34.361475Z",
     "start_time": "2022-05-10T17:54:34.346958Z"
    }
   },
   "outputs": [],
   "source": [
    "dataFolder_path = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7876da2",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a02fd7c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:05:53.167267Z",
     "start_time": "2022-05-10T18:05:53.159274Z"
    }
   },
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(treebank_tag):\n",
    "#     '''\n",
    "#     Translate nltk POS to wordnet tags\n",
    "#     '''\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return wordnet.NOUN\n",
    "\n",
    "\n",
    "def doc_preparer(doc, stem = False, stop_words=sw, implement_transtranslate = False):\n",
    "    '''\n",
    "\n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    #Stemming seems to work better. Lemming can't identify plurals of products\n",
    "    \n",
    "    \n",
    "#     lemmed_keywords = ['apple',\n",
    "#                 'ipad', 'ipads',\n",
    "#                 'iphone', 'iphones',\n",
    "#                 'itunes',\n",
    "#                 'google', 'googled',\n",
    "#                 'android', 'droid', 'androids', 'droids',\n",
    "#                 'circle', 'circles'\n",
    "#                 'app', 'apps']\n",
    "\n",
    "#     stemmed_keywords = ['appl',\n",
    "#                         'ipad',\n",
    "#                         'iphon',\n",
    "#                         'itun',\n",
    "#                         'googl',\n",
    "#                         'android',\n",
    "#                         'droid',\n",
    "#                         'circl',\n",
    "#                         'app']\n",
    "\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "#     doc = pos_tag(doc)\n",
    "#     doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "#     doc = [word for word in doc if word in lemmed_keywords]\n",
    "\n",
    "#     if implement_transtranslate:\n",
    "#         doc = \n",
    "   \n",
    "    p_stemmer = nltk.stem.PorterStemmer()\n",
    "    if stem:\n",
    "        doc = [p_stemmer.stem(word) for word in doc]\n",
    "    return ' '.join(doc)\n",
    "\n",
    "\n",
    "def cv_printScores(cv_metric):\n",
    "    print('CV Results')\n",
    "    print('='*32)\n",
    "    print('Accuracy')\n",
    "    print('-'*32)\n",
    "    print(f\"Training accuracy: {cv_metric['train_accuracy'].mean():.3f}\")\n",
    "    print(f\"Test accuracy:     {cv_metric['test_accuracy'].mean():.3f}\")\n",
    "    print('F-1 Score')\n",
    "    print('-'*32)\n",
    "    print(f\"Training F1 score: {cv_metric['train_f1_macro'].mean():.3f}\")\n",
    "    print(f\"Test F1 score:     {cv_metric['test_f1_macro'].mean():.3f}\")\n",
    "    \n",
    "\n",
    "    \n",
    "def getTopWordFreq(df,col,n):\n",
    "    '''\n",
    "    generates FreqDist\n",
    "    df: dataframe\n",
    "    col: column you want to run a freqDist on\n",
    "    n: number of most common items    \n",
    "    '''\n",
    "    \n",
    "#     pos_df = data_df[data_df['is_there_an_emotion_directed_at_a_brand_or_product']=='Positive emotion']\n",
    "    word_freq = FreqDist()\n",
    "    for text in df[col].map(lambda x:doc_preparer(x,stem=False)):\n",
    "        for word in text.split():\n",
    "            word_freq[word] +=1\n",
    "    return word_freq.most_common(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ab6c08fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:54:34.820067Z",
     "start_time": "2022-05-10T17:54:34.806064Z"
    }
   },
   "outputs": [],
   "source": [
    "# from googletrans import Translator\n",
    "# translator = Translator()\n",
    "\n",
    "# def German_translation(x):\n",
    "#     print(x)    \n",
    "#     german_translation = translator.translate(x, dest='de')    \n",
    "#     return german_translation.text\n",
    "\n",
    "# def English_translation(x):\n",
    "#     print(x)    \n",
    "#     english_translation = translator.translate(x, dest='en')    \n",
    "#     return english_translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7ea6ad7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:54:35.119969Z",
     "start_time": "2022-05-10T17:54:35.109967Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Original Author: Jason Wei\n",
    "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks:\n",
    "https://github.com/jasonwei20/eda_nlp/blob/04ab29c5b18d2d72f9fa5b304322aaf4793acea0/code/eda.py#L86\n",
    "\n",
    "'''\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "#             print(l.name())\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\n",
    "            if (synonym in sw):\n",
    "                pass\n",
    "            synonyms.add(synonym) \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "def synonym_augmentation(sentence, numOfWordsToSyn=1, numOfExtraSentences=2):\n",
    "    '''\n",
    "    sentence: string to augment using synomization\n",
    "    numOfWordsToSyn: number of words in the sentence to synonymize\n",
    "    numOfExtraSentences: number of augmented sentences to return. \n",
    "    '''\n",
    "    new_sentences = []\n",
    "    sentence = sentence.lower()\n",
    "    original_words = sentence.split()\n",
    "    \n",
    "    if len(original_words)<numOfWordsToSyn:\n",
    "        numOfWordsToSyn = len(original_words)\n",
    "        \n",
    "    random_word_list = list(set(original_words))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    # Randomly pick a word in list \n",
    "    # Pick a random synonym that is not in sw list\n",
    "    # Replace that word in our list of words    \n",
    "    for i in range(0,numOfExtraSentences):\n",
    "        new_words = original_words\n",
    "        for random_word in random_word_list:\n",
    "            synonyms = get_synonyms(random_word)\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(list(synonyms))\n",
    "\n",
    "                #new_words is rebuilt with synonym replacement, while maintaining order of words\n",
    "                new_words = [synonym if word == random_word else word for word in new_words]\n",
    "                random_word_list.remove(random_word)\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= numOfWordsToSyn: #only replace up to n words\n",
    "                break\n",
    "                \n",
    "        #Form new sentence by joining new words\n",
    "        s = ' '.join(new_words).lower().strip().replace('  ', ' ')\n",
    "        \n",
    "        # add sentences to our list of synonymized sentences\n",
    "        # if not already there and not same as the original sentence\n",
    "        if (s not in new_sentences) & (s!=sentence):\n",
    "            new_sentences.append(s)\n",
    "\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0a887",
   "metadata": {},
   "source": [
    "# Marshall's sw augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d47dbc5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:18:51.541781Z",
     "start_time": "2022-05-10T18:18:51.511775Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 561),\n",
       " ('mention', 305),\n",
       " ('ipad', 204),\n",
       " ('quot', 169),\n",
       " ('iphone', 158)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df = data_df[data_df['is_there_an_emotion_directed_at_a_brand_or_product']=='Negative emotion']\n",
    "getTopWordFreq(neg_df, 'tweet_text',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f9599260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:18:52.094603Z",
     "start_time": "2022-05-10T18:18:51.976566Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 3037),\n",
       " ('mention', 2153),\n",
       " ('ipad', 1201),\n",
       " ('link', 1170),\n",
       " ('rt', 933)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = data_df[data_df['is_there_an_emotion_directed_at_a_brand_or_product']=='Positive emotion']\n",
    "getTopWordFreq(pos_df, 'tweet_text',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "99143de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:18:59.852484Z",
     "start_time": "2022-05-10T18:18:59.649294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sxsw', 5518),\n",
       " ('mention', 4393),\n",
       " ('link', 2813),\n",
       " ('rt', 1856),\n",
       " ('google', 1652)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neu_df = data_df[data_df['is_there_an_emotion_directed_at_a_brand_or_product']=='No emotion toward brand or product']\n",
    "getTopWordFreq(neu_df, 'tweet_text',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fdec695f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:25:23.670243Z",
     "start_time": "2022-05-10T18:25:23.537213Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['quot',\n",
       " 'austin',\n",
       " 'amp',\n",
       " 'google',\n",
       " 'launch',\n",
       " 'android',\n",
       " 'get',\n",
       " 'sxsw',\n",
       " 'new',\n",
       " 'iphone',\n",
       " 'app',\n",
       " 'mention',\n",
       " 'one',\n",
       " 'apple',\n",
       " 'ipad',\n",
       " 'store',\n",
       " 'link',\n",
       " 'rt']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_Common_Words = 25\n",
    "common_neg_words = [tup[0] for tup in getTopWordFreq(neg_df, 'tweet_text',num_Common_Words)]\n",
    "common_pos_words = [tup[0] for tup in getTopWordFreq(pos_df, 'tweet_text',num_Common_Words)]\n",
    "# common_neu_words = [tup[0] for tup in getTopWordFreq(neu_df, 'tweet_text',num_Common_Words)]\n",
    "new_sw_words = list(set(common_neg_words).intersection(set(common_pos_words)))\n",
    "print(len(new_sw_words))\n",
    "new_sw_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9e2d2825",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T18:25:28.888739Z",
     "start_time": "2022-05-10T18:25:28.577568Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for each in [tup[0] for tup in getTopWordFreq(data_df, 'tweet_text',18)]:\n",
    "    print(each in list(set(common_neg_words).intersection(set(common_pos_words)).intersection(set(common_neu_words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0fd40fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:19.751813Z",
     "start_time": "2022-05-10T17:49:19.737328Z"
    }
   },
   "outputs": [],
   "source": [
    "sw.extend(list(set(neg_keys).intersection(pos_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9bf45b32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.425640Z",
     "start_time": "2022-05-10T17:49:21.411637Z"
    }
   },
   "outputs": [],
   "source": [
    "#Maybe don't add mention? and link?\n",
    "# sw.extend(['sxsw','rt','quot','austin','sxswi',\n",
    "#            'mention','link',\n",
    "#            'today','w'\n",
    "#           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "601621e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:19.799832Z",
     "start_time": "2022-05-10T17:49:19.768822Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(dataFolder_path+'judge_1377884607_tweet_product_company.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b29a914f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:19.879850Z",
     "start_time": "2022-05-10T17:49:19.800832Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datframe has 8721 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Info Table:</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Zeroes</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Nulls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Uniques</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Missing/Unknown</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Details:</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Columns:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweet_text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01 %</td>\n",
       "      <td>8694</td>\n",
       "      <td>99.69 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>5552</td>\n",
       "      <td>63.66 %</td>\n",
       "      <td>10</td>\n",
       "      <td>0.11 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Info Table:                                        Zeroes          Nulls  \\\n",
       "Details:                                            Count Fraction Count   \n",
       "Columns:                                                                   \n",
       "tweet_text                                              0   0.00 %     1   \n",
       "emotion_in_tweet_is_directed_at                         0   0.00 %  5552   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product      0   0.00 %     0   \n",
       "\n",
       "Info Table:                                                 Uniques           \\\n",
       "Details:                                           Fraction   Count Fraction   \n",
       "Columns:                                                                       \n",
       "tweet_text                                           0.01 %    8694  99.69 %   \n",
       "emotion_in_tweet_is_directed_at                     63.66 %      10   0.11 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product   0.00 %       4   0.05 %   \n",
       "\n",
       "Info Table:                                        Missing/Unknown           \\\n",
       "Details:                                                     Count Fraction   \n",
       "Columns:                                                                      \n",
       "tweet_text                                                       0   0.00 %   \n",
       "emotion_in_tweet_is_directed_at                                  0   0.00 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product               0   0.00 %   \n",
       "\n",
       "Info Table:                                        Mean Median  \n",
       "Details:                                                        \n",
       "Columns:                                                        \n",
       "tweet_text                                          0.0    0.0  \n",
       "emotion_in_tweet_is_directed_at                     0.0    0.0  \n",
       "is_there_an_emotion_directed_at_a_brand_or_product  0.0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataFrame_info(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b8eb3",
   "metadata": {},
   "source": [
    "Looking at the 1 null in tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f05431b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:19.895854Z",
     "start_time": "2022-05-10T17:49:19.880851Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_text emotion_in_tweet_is_directed_at  \\\n",
       "6        NaN                             NaN   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "6                 No emotion toward brand or product  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df.tweet_text.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24ca7a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:19.911293Z",
     "start_time": "2022-05-10T17:49:19.896855Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df.dropna(subset=['tweet_text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "675d5b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:19.927300Z",
     "start_time": "2022-05-10T17:49:19.912293Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8720, 3)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ab20409",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.347622Z",
     "start_time": "2022-05-10T17:49:19.928301Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datframe has 8720 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Info Table:</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Zeroes</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Nulls</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Uniques</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Missing/Unknown</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Details:</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th>Count</th>\n",
       "      <th>Fraction</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Columns:</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweet_text</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>8693</td>\n",
       "      <td>99.69 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>5551</td>\n",
       "      <td>63.66 %</td>\n",
       "      <td>10</td>\n",
       "      <td>0.11 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>4</td>\n",
       "      <td>0.05 %</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00 %</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Info Table:                                        Zeroes          Nulls  \\\n",
       "Details:                                            Count Fraction Count   \n",
       "Columns:                                                                   \n",
       "tweet_text                                              0   0.00 %     0   \n",
       "emotion_in_tweet_is_directed_at                         0   0.00 %  5551   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product      0   0.00 %     0   \n",
       "\n",
       "Info Table:                                                 Uniques           \\\n",
       "Details:                                           Fraction   Count Fraction   \n",
       "Columns:                                                                       \n",
       "tweet_text                                           0.00 %    8693  99.69 %   \n",
       "emotion_in_tweet_is_directed_at                     63.66 %      10   0.11 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product   0.00 %       4   0.05 %   \n",
       "\n",
       "Info Table:                                        Missing/Unknown           \\\n",
       "Details:                                                     Count Fraction   \n",
       "Columns:                                                                      \n",
       "tweet_text                                                       0   0.00 %   \n",
       "emotion_in_tweet_is_directed_at                                  0   0.00 %   \n",
       "is_there_an_emotion_directed_at_a_brand_or_product               0   0.00 %   \n",
       "\n",
       "Info Table:                                        Mean Median  \n",
       "Details:                                                        \n",
       "Columns:                                                        \n",
       "tweet_text                                          0.0    0.0  \n",
       "emotion_in_tweet_is_directed_at                     0.0    0.0  \n",
       "is_there_an_emotion_directed_at_a_brand_or_product  0.0    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataFrame_info(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d67f45",
   "metadata": {},
   "source": [
    "Lets look at the emotion quotient column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "591fca67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.363625Z",
     "start_time": "2022-05-10T17:49:21.348622Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No emotion toward brand or product    5155\n",
       "Positive emotion                      2869\n",
       "Negative emotion                       545\n",
       "I can't tell                           151\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c4048",
   "metadata": {},
   "source": [
    "Major class imbalance. Should consider dropping \"I can't tell\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "efa191cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.379629Z",
     "start_time": "2022-05-10T17:49:21.368626Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data_df[data_df.is_there_an_emotion_directed_at_a_brand_or_product == \"I can't tell\" ]\n",
    "\n",
    "data_df = data_df[data_df.is_there_an_emotion_directed_at_a_brand_or_product != \"I can't tell\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bfdbe87a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.394633Z",
     "start_time": "2022-05-10T17:49:21.380630Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8569, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3da51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-09T02:36:50.450837Z",
     "start_time": "2022-05-09T02:36:50.432834Z"
    }
   },
   "source": [
    "Lets look at some of the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fc43408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.409636Z",
     "start_time": "2022-05-10T17:49:21.395633Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_freq = FreqDist()\n",
    "# for tweet in data_df['tweet_text'].map(lambda x:doc_preparer(x,stem=False)):\n",
    "#     for word in tweet.split():\n",
    "#         word_freq[word] +=1\n",
    "# word_freq.most_common(n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccffee",
   "metadata": {},
   "source": [
    "Adding venue specific words and twitter specific words to stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0103baa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.425640Z",
     "start_time": "2022-05-10T17:49:21.411637Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Maybe don't add mention? and link?\n",
    "# sw.extend(['sxsw','rt','quot','austin','sxswi',\n",
    "#            'mention','link',\n",
    "#            'today','w'\n",
    "#           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bed540e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:21.441643Z",
     "start_time": "2022-05-10T17:49:21.426639Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_freq = FreqDist()\n",
    "# for tweet in data_df['tweet_text'].map(lambda x:doc_preparer(x,stem=True)):\n",
    "#     for word in tweet.split():\n",
    "#         word_freq[word] +=1\n",
    "# word_freq.most_common(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9e4fd93a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:22.451621Z",
     "start_time": "2022-05-10T17:49:21.442643Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df['stemmed_tokens'] = data_df['tweet_text'].map(lambda x:doc_preparer(x,stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bfb76f15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:22.467625Z",
     "start_time": "2022-05-10T17:49:22.454623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley g hr tweet rise dead need upgrad plugin station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessede know fludapp awesom like appreci design also give free ts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait also sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festiv crashi year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer tim reilli tech book confer matt mullenweg wordpress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>everywher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>wave buzz interrupt regularli schedul geek program big news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &amp;quot;We're operating w/out data.&amp;quot; #sxsw #health2dev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>zeiger physician never report potenti ae yet fda reli physician oper data health dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>verizon custom complain fell back hour weekend cours yorker attend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>�ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>test check offer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet_text  \\\n",
       "0                       .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1           @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                                       @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                                    @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4                   @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "...                                                                                                                                                 ...   \n",
       "8716                                                                                                                      Ipad everywhere. #SXSW {link}   \n",
       "8717                      Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles   \n",
       "8718  Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev   \n",
       "8719       Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.   \n",
       "8720                                                  �ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "8716                            iPad   \n",
       "8717                             NaN   \n",
       "8718                             NaN   \n",
       "8719                             NaN   \n",
       "8720                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                      Negative emotion   \n",
       "1                                      Positive emotion   \n",
       "2                                      Positive emotion   \n",
       "3                                      Negative emotion   \n",
       "4                                      Positive emotion   \n",
       "...                                                 ...   \n",
       "8716                                   Positive emotion   \n",
       "8717                 No emotion toward brand or product   \n",
       "8718                 No emotion toward brand or product   \n",
       "8719                 No emotion toward brand or product   \n",
       "8720                 No emotion toward brand or product   \n",
       "\n",
       "                                                                                    stemmed_tokens  \n",
       "0                                           wesley g hr tweet rise dead need upgrad plugin station  \n",
       "1                                jessede know fludapp awesom like appreci design also give free ts  \n",
       "2                                                                        swonderlin wait also sale  \n",
       "3                                                                     hope year festiv crashi year  \n",
       "4     sxtxstate great stuff fri marissa mayer tim reilli tech book confer matt mullenweg wordpress  \n",
       "...                                                                                            ...  \n",
       "8716                                                                                     everywher  \n",
       "8717                                   wave buzz interrupt regularli schedul geek program big news  \n",
       "8718          zeiger physician never report potenti ae yet fda reli physician oper data health dev  \n",
       "8719                            verizon custom complain fell back hour weekend cours yorker attend  \n",
       "8720                                                                              test check offer  \n",
       "\n",
       "[8569 rows x 4 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3baaef",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Label Encoding the sentinment column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1c279ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:33.208817Z",
     "start_time": "2022-05-10T17:49:33.197814Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'No emotion toward brand or product',\n",
       "       'Positive emotion'], dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "data_df['sentiment_target'] = le.fit_transform(data_df.is_there_an_emotion_directed_at_a_brand_or_product)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6903ec0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:34.104819Z",
     "start_time": "2022-05-10T17:49:34.093817Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>sentiment_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>wesley g hr tweet rise dead need upgrad plugin station</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>jessede know fludapp awesom like appreci design also give free ts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>swonderlin wait also sale</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>hope year festiv crashi year</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>sxtxstate great stuff fri marissa mayer tim reilli tech book confer matt mullenweg wordpress</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>everywher</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8717</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>wave buzz interrupt regularli schedul geek program big news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8718</th>\n",
       "      <td>Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &amp;quot;We're operating w/out data.&amp;quot; #sxsw #health2dev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>zeiger physician never report potenti ae yet fda reli physician oper data health dev</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8719</th>\n",
       "      <td>Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>verizon custom complain fell back hour weekend cours yorker attend</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720</th>\n",
       "      <td>�ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>test check offer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8569 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet_text  \\\n",
       "0                       .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1           @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                                       @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                                    @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4                   @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "...                                                                                                                                                 ...   \n",
       "8716                                                                                                                      Ipad everywhere. #SXSW {link}   \n",
       "8717                      Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles   \n",
       "8718  Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev   \n",
       "8719       Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.   \n",
       "8720                                                  �ϡ�����_��ʋ�΋�ҋ�������⋁_��������_���RT @mention Google Tests ���Check-in Offers�۝ At #SXSW {link}   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "8716                            iPad   \n",
       "8717                             NaN   \n",
       "8718                             NaN   \n",
       "8719                             NaN   \n",
       "8720                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                      Negative emotion   \n",
       "1                                      Positive emotion   \n",
       "2                                      Positive emotion   \n",
       "3                                      Negative emotion   \n",
       "4                                      Positive emotion   \n",
       "...                                                 ...   \n",
       "8716                                   Positive emotion   \n",
       "8717                 No emotion toward brand or product   \n",
       "8718                 No emotion toward brand or product   \n",
       "8719                 No emotion toward brand or product   \n",
       "8720                 No emotion toward brand or product   \n",
       "\n",
       "                                                                                    stemmed_tokens  \\\n",
       "0                                           wesley g hr tweet rise dead need upgrad plugin station   \n",
       "1                                jessede know fludapp awesom like appreci design also give free ts   \n",
       "2                                                                        swonderlin wait also sale   \n",
       "3                                                                     hope year festiv crashi year   \n",
       "4     sxtxstate great stuff fri marissa mayer tim reilli tech book confer matt mullenweg wordpress   \n",
       "...                                                                                            ...   \n",
       "8716                                                                                     everywher   \n",
       "8717                                   wave buzz interrupt regularli schedul geek program big news   \n",
       "8718          zeiger physician never report potenti ae yet fda reli physician oper data health dev   \n",
       "8719                            verizon custom complain fell back hour weekend cours yorker attend   \n",
       "8720                                                                              test check offer   \n",
       "\n",
       "      sentiment_target  \n",
       "0                    0  \n",
       "1                    2  \n",
       "2                    2  \n",
       "3                    0  \n",
       "4                    2  \n",
       "...                ...  \n",
       "8716                 2  \n",
       "8717                 1  \n",
       "8718                 1  \n",
       "8719                 1  \n",
       "8720                 1  \n",
       "\n",
       "[8569 rows x 5 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a57a11",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76321443",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8dfae5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6adb46d4",
   "metadata": {},
   "source": [
    "# Define X,y, train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe31240d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:44.539043Z",
     "start_time": "2022-05-10T17:49:44.534050Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data_df['stemmed_tokens']\n",
    "y = data_df['sentiment_target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "450a51a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:38.109104Z",
     "start_time": "2022-05-10T17:49:38.086105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major call possibl smm gt gt realli\n",
      "['john major phone possibl smm gt gt realli']\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(X_train, y_train, right_index = True,\n",
    "               left_index = True)\n",
    "df[df['sentiment_target'] == 0]\n",
    "for each in (df[df['sentiment_target'] == 0]).stemmed_tokens:\n",
    "    print(each)\n",
    "    print(synonym_augmentation(each,numOfWordsToSyn=5))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "42635827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:14:36.144734Z",
     "start_time": "2022-05-10T17:14:36.130730Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3622\n",
       "2    1999\n",
       "0     377\n",
       "Name: sentiment_target, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7b78ea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:14:37.847758Z",
     "start_time": "2022-05-10T17:14:37.842757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3207                                                            meet guy bought first\n",
       "4257                                                 go share photo video ireport cnn\n",
       "6323                                                               call possibl sxswi\n",
       "1365                    gamif design mobil bank market artikelen dossier event mediap\n",
       "8085    checkout tronloungesxsw disney nd screen free tronlegaci r conf gur soundtrax\n",
       "                                            ...                                      \n",
       "5174                      anyon know popup close tonit sxtxstate someon said midnight\n",
       "2370                     ballroom marissagoogl talk cool project obv love art project\n",
       "7870                                                               major call possibl\n",
       "5240                                                       downtown open til midnight\n",
       "4732                                                                     parti awesom\n",
       "Name: stemmed_tokens, Length: 5998, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025cae",
   "metadata": {},
   "source": [
    "# CountVec with MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e36304c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:51.763444Z",
     "start_time": "2022-05-10T17:49:51.685902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb885734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:49:59.541661Z",
     "start_time": "2022-05-10T17:49:55.373724Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.832\n",
      "Test accuracy:     0.654\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.756\n",
      "Test F1 score:     0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    4.0s finished\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec,y_train)\n",
    "cvec_mnb_cvResults = cross_validate(mnb,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      cv=4,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(cvec_mnb_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93010be9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:15.997683Z",
     "start_time": "2022-05-10T17:09:15.540580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713341112407624"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec = cvec.transform(X_test)\n",
    "X_test_vec = pd.DataFrame.sparse.from_spmatrix(X_test_vec)\n",
    "X_test_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_test_vec.set_index(y_test.index, inplace=True)\n",
    "\n",
    "mnb.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9053e23",
   "metadata": {},
   "source": [
    "# TiffyDiffy with MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2356535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:15:33.269812Z",
     "start_time": "2022-05-10T17:15:33.179790Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(tvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1641a2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:15:38.019299Z",
     "start_time": "2022-05-10T17:15:33.915962Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.753\n",
      "Test accuracy:     0.648\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.492\n",
      "Test F1 score:     0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    3.9s finished\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_vec,y_train)\n",
    "tvec_mnb_cvResults = cross_validate(mnb,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      cv=4,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(tvec_mnb_cvResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18944db7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T15:48:34.339194Z",
     "start_time": "2022-05-10T15:48:34.330192Z"
    }
   },
   "source": [
    "# CountVec with RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bded004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:16:06.976875Z",
     "start_time": "2022-05-10T17:16:06.900236Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "\n",
    "X_train_vec = cvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64d3c4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:16:24.206830Z",
     "start_time": "2022-05-10T17:16:07.610021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.833\n",
      "Test accuracy:     0.647\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.757\n",
      "Test F1 score:     0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:   11.6s finished\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vec,y_train)\n",
    "cvec_rfc_cvResults = cross_validate(rfc,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      cv=4,\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(cvec_mnb_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b404205f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:37.794474Z",
     "start_time": "2022-05-10T17:09:37.157460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966161026837806"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_vec = cvec.transform(X_test)\n",
    "X_test_vec = pd.DataFrame.sparse.from_spmatrix(X_test_vec)\n",
    "X_test_vec.columns = sorted(cvec.vocabulary_)\n",
    "X_test_vec.set_index(y_test.index, inplace=True)\n",
    "\n",
    "rfc.score(X_test_vec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c219744",
   "metadata": {},
   "source": [
    "# TiffyDiffy with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0a643d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:38.433287Z",
     "start_time": "2022-05-10T17:09:38.358270Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(tvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "188c910a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:53.198040Z",
     "start_time": "2022-05-10T17:09:38.907977Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.971\n",
      "Test accuracy:     0.676\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.967\n",
      "Test F1 score:     0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   4 out of   4 | elapsed:    9.9s finished\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_vec,y_train)\n",
    "tvec_rfc_cvResults = cross_validate(rfc,\n",
    "                                    X_train_vec,\n",
    "                                    y_train,\n",
    "                                    scoring=('accuracy', 'f1_macro'),\n",
    "                                    cv=4,\n",
    "                                    verbose=1,\n",
    "                                    n_jobs=NB_n_jobs,\n",
    "                                    return_train_score=True)\n",
    "\n",
    "cv_printScores(tvec_rfc_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1dca96d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:53.356368Z",
     "start_time": "2022-05-10T17:09:53.341361Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "# params['rfc__criterion'] = ['gini','entropy']\n",
    "# params['rfc__n_estimators'] = np.arange(50,250,50)\n",
    "# params['rfc__max_depth'] = np.arange(150,200,10)\n",
    "# # params['rfc__max_leaf_nodes']=[4000,4500,5000]\n",
    "\n",
    "# rfc_model_pipe = Pipeline([\n",
    "#     ('vec',TfidfVectorizer()),\n",
    "#    ('rfc',RandomForestClassifier(random_state=42,n_jobs=NB_n_jobs))\n",
    "# ])\n",
    "\n",
    "# rfc_gs1 = GridSearchCV(estimator=rfc_model_pipe,\n",
    "#                            param_grid=params,\n",
    "#                            n_jobs=NB_n_jobs,\n",
    "#                            scoring=[ 'accuracy','precision_macro','recall_macro','f1_macro'],\n",
    "#                            refit='accuracy',\n",
    "#                            return_train_score=True)\n",
    "\n",
    "# rfc_gs1.fit(X_train,y_train)\n",
    "# prettyPrintGridCVResults(rfc_gs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13814e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:53.812457Z",
     "start_time": "2022-05-10T17:09:53.800482Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_gs1.best_estimator_.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa965e30",
   "metadata": {},
   "source": [
    "# TiffyDiffy with GradientBoosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1c19415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:09:54.124964Z",
     "start_time": "2022-05-10T17:09:54.036786Z"
    }
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "\n",
    "X_train_vec = tvec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame.sparse.from_spmatrix(X_train_vec)\n",
    "X_train_vec.columns = sorted(tvec.vocabulary_)\n",
    "X_train_vec.set_index(y_train.index, inplace=True)\n",
    "# X_train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ba3b1f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-10T17:10:11.137287Z",
     "start_time": "2022-05-10T17:09:54.945330Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 15 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results\n",
      "================================\n",
      "Accuracy\n",
      "--------------------------------\n",
      "Training accuracy: 0.737\n",
      "Test accuracy:     0.660\n",
      "F-1 Score\n",
      "--------------------------------\n",
      "Training F1 score: 0.578\n",
      "Test F1 score:     0.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done   5 out of   5 | elapsed:   12.5s finished\n"
     ]
    }
   ],
   "source": [
    "grad = GradientBoostingClassifier()\n",
    "grad.fit(X_train_vec,y_train)\n",
    "tvec_grad_cvResults = cross_validate(grad,\n",
    "                                      X_train_vec,\n",
    "                                      y_train,\n",
    "                                      scoring=('accuracy', 'f1_macro'),\n",
    "                                      verbose=1,\n",
    "                                      n_jobs = NB_n_jobs,\n",
    "                                      return_train_score=True)\n",
    "\n",
    "cv_printScores(tvec_grad_cvResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc0c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ceae97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfGPU",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
